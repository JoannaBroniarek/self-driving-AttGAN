{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "2mclR5a8b5Yy",
    "outputId": "e4ae71f5-ea8b-4637-aef0-44aa0bb38099"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "from tensorflow.keras.backend import set_session\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "config = tf.ConfigProto()\n",
    "# dynamically grow the memory used on the GPU\n",
    "config.gpu_options.allow_growth = True\n",
    "# to log device placement (on which device the operation ran)\n",
    "config.log_device_placement = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "# (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "# set this TensorFlow session as the default session for Keras\n",
    "set_session(sess)\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1PypnjZsFjMq"
   },
   "outputs": [],
   "source": [
    "#!wget 'https://gitlab.com/federicozzo/attgan/raw/master/resources/train.tfrecords.zip'\n",
    "#!wget 'https://gitlab.com/federicozzo/attgan/raw/master/resources/test.tfrecords.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkcEHJD1I3sh"
   },
   "outputs": [],
   "source": [
    "#!unzip train.tfrecords.zip\n",
    "#!unzip test.tfrecords.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmuzyJNdJB90"
   },
   "outputs": [],
   "source": [
    "#!mkdir resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTHINMR7DoBD"
   },
   "outputs": [],
   "source": [
    "#!mv 'test.tfrecords' resources\n",
    "#!mv 'train.tfrecords' resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNnS3HMbJr3i"
   },
   "outputs": [],
   "source": [
    "#!rm -f 'test.tfrecords.zip'\n",
    "#!rm -f 'train.tfrecords.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wFYaYF6dV6S"
   },
   "outputs": [],
   "source": [
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "BATCH_SIZE = 512\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhcCpvdZA4Ks"
   },
   "source": [
    "# data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfgcpxcuA4Ku"
   },
   "outputs": [],
   "source": [
    "class Barkley_Deep_Drive(object):\n",
    "    \n",
    "    def __init__(self, tfrecord_path):\n",
    "        self.dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def normalize(image, label):\n",
    "        \"\"\"Convert `image` from [0, 255] -> [-0.5, 0.5] floats.\"\"\"\n",
    "        image = tf.cast(image, tf.float32) * (1. / 255)\n",
    "        return image, label\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(serialized_example):\n",
    "        \"\"\"\n",
    "        Parses an image and label from the given `serialized_example`.\n",
    "        It is used as a map function for `dataset.map`\n",
    "        \"\"\"\n",
    "        IMAGE_SHAPE = (IMG_HEIGHT,IMG_WIDTH,3)\n",
    "\n",
    "        # 1. define a parser\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            # Defaults are not specified since both keys are required.\n",
    "            features={\n",
    "                'image': tf.FixedLenFeature([], tf.string),\n",
    "                'label': tf.FixedLenFeature([], tf.string),\n",
    "            })\n",
    "\n",
    "        # 2. Convert the data\n",
    "        image = tf.decode_raw(features['image'], tf.float32)\n",
    "        label = features['label']\n",
    "\n",
    "        # 3. reshape\n",
    "        image = tf.convert_to_tensor(tf.reshape(image, IMAGE_SHAPE))\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def get_batch(self, batch_size = 32, shuffle = True, num_threads = -1, buffer_size=4096):\n",
    "        \n",
    "        self.dataset = self.dataset.map(self.decode, num_parallel_calls=num_threads)\n",
    "        \n",
    "        if shuffle:\n",
    "            self.dataset = self.dataset.shuffle(buffer_size)\n",
    "            \n",
    "        self.dataset = self.dataset.map(self.normalize)\n",
    "        self.dataset = self.dataset.batch(batch_size)\n",
    "        \n",
    "        iterator = self.dataset.make_one_shot_iterator()\n",
    "\n",
    "        image_iterator, label_iterator = iterator.get_next()\n",
    "\n",
    "        return image_iterator, label_iterator\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSOryRbfA4Kw"
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8rRvaSccLA9"
   },
   "outputs": [],
   "source": [
    "def discriminator(input_var, name='D_discriminator', reuse=None):\n",
    "    with tf.variable_scope(name,reuse=reuse):\n",
    "        leakyrelu_alpha = 0.2\n",
    "        num_blocks = 5\n",
    "        filters = 64\n",
    "        kernel_size = 4\n",
    "        strides = 2\n",
    "        # Five intermediate blocks : conv + layer norm + instance norm + leaky relu\n",
    "        for i in range(num_blocks):\n",
    "            conv = tf.layers.conv2d(inputs = input_var,\n",
    "                                    filters = filters, \n",
    "                                    kernel_size = kernel_size,\n",
    "                                    padding = 'valid', \n",
    "                                    strides = strides)\n",
    "            layer_norm = tf.contrib.layers.layer_norm(conv)\n",
    "            instance_norm = tf.contrib.layers.instance_norm(layer_norm)\n",
    "            leaky_relu_out = tf.nn.leaky_relu(instance_norm, alpha = leakyrelu_alpha)\n",
    "\n",
    "            input_var = leaky_relu_out\n",
    "            filters += filters\n",
    "            \n",
    "        \n",
    "        # Output block : fc(1024) + layer norm + instance norm + leaky relu\n",
    "        output_blocks = input_var\n",
    "        \n",
    "        fc = tf.contrib.layers.fully_connected(output_blocks, num_outputs = 1024)\n",
    "        layer_norm = tf.contrib.layers.layer_norm(fc)\n",
    "        instance_norm = tf.contrib.layers.instance_norm(layer_norm)\n",
    "        leaky_relu_out = tf.nn.leaky_relu(instance_norm, alpha = leakyrelu_alpha)\n",
    "        # Output\n",
    "        output = tf.contrib.layers.fully_connected(leaky_relu_out, num_outputs = 1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cd9Q0Op6j78e"
   },
   "outputs": [],
   "source": [
    "def classifier(input_var, name='C_classifier', reuse=None):\n",
    "    with tf.variable_scope(name,reuse=reuse):\n",
    "        leakyrelu_alpha = 0.2\n",
    "        num_blocks = 5\n",
    "        filters = 64\n",
    "        kernel_size = 4\n",
    "        strides = 2\n",
    "        # Five intermediate blocks : conv + layer norm + instance norm + leaky relu\n",
    "        for i in range(num_blocks):\n",
    "            \n",
    "            conv = tf.layers.conv2d(inputs = input_var,\n",
    "                                    filters = filters, \n",
    "                                    kernel_size = kernel_size,\n",
    "                                    padding = 'valid', \n",
    "                                    strides = strides)\n",
    "            layer_norm = tf.contrib.layers.layer_norm(conv)\n",
    "            instance_norm = tf.contrib.layers.instance_norm(layer_norm)\n",
    "            leaky_relu_out = tf.nn.leaky_relu(instance_norm, alpha = leakyrelu_alpha)\n",
    "\n",
    "            input_var = leaky_relu_out\n",
    "            filters += filters\n",
    "\n",
    "        # Output block : fc(1024) + layer norm + instance norm + leaky relu\n",
    "        output_blocks = input_var\n",
    "        fc = tf.contrib.layers.fully_connected(output_blocks, \n",
    "                                               num_outputs = 1024)\n",
    "        layer_norm = tf.contrib.layers.layer_norm(fc)\n",
    "        instance_norm = tf.contrib.layers.instance_norm(layer_norm)\n",
    "        leaky_relu_out = tf.nn.leaky_relu(instance_norm, alpha = leakyrelu_alpha)\n",
    "\n",
    "        # Output\n",
    "        out = tf.contrib.layers.fully_connected(leaky_relu_out,\n",
    "                                                num_outputs=2)\n",
    "        \n",
    "        return tf.nn.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5odAYc69lUV"
   },
   "outputs": [],
   "source": [
    "def conv2d(input_, output_dim, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=\"conv2d\", padding = 'SAME'):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
    "              initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        \n",
    "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=padding)\n",
    "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        return tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
    "\n",
    "    \n",
    "def deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, name=\"deconv2d\", stddev=0.02, with_w=False):\n",
    "    with tf.variable_scope(name):\n",
    "        # filter : [height, width, output_channels, in_channels]\n",
    "        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
    "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "\n",
    "        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        return tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
    "\n",
    "\n",
    "    \n",
    "def encoder(inputs, name = 'G_encoder', reuse=tf.AUTO_REUSE, is_training = True):\n",
    "    \"\"\"\n",
    "    encoder function\n",
    "    :param: inputs\n",
    "    :param: name\n",
    "    :return list of layers:\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        \n",
    "        leakyrelu_alpha = 0.2\n",
    "        num_blocks = 5\n",
    "        filters = 64\n",
    "        kernel_size = 4\n",
    "        strides = 2\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(inputs)\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            conv = conv2d(inputs, filters, kernel_size, kernel_size, strides, strides, name = str(i+1))\n",
    "            batch_norm = tf.contrib.layers.batch_norm(conv)\n",
    "            leaky_relu = tf.nn.leaky_relu(batch_norm, alpha = leakyrelu_alpha)\n",
    "            \n",
    "            inputs = leaky_relu\n",
    "            filters += filters\n",
    "            layers.append(inputs)\n",
    "            \n",
    "        return layers\n",
    "    \n",
    "def decoder(inputs, label, name = 'G_decoder', reuse=None, is_training = True):\n",
    "    \"\"\"\n",
    "    decoder function\n",
    "    :param: inputs (list of layers from encoder)\n",
    "    :param: name\n",
    "    :return tanh(conv5):\n",
    "    \"\"\"\n",
    "    leakyrelu_alpha = 0.2\n",
    "    filters = 1024\n",
    "    kernel_size = 4\n",
    "    strides = 2\n",
    "    \n",
    "    input_ = inputs[-1]\n",
    "\n",
    "    def _attribute_concat(label, z):\n",
    "        label = tf.expand_dims(label, 1)\n",
    "        label = tf.expand_dims(label, 1)\n",
    "        #label = label[:,tf.newaxis, tf.newaxis,:] #or use expand_dims twice\n",
    "        label = tf.tile(label, [1, *z.get_shape().as_list()[1:3], 1])\n",
    "        label = tf.cast(label, dtype=tf.float32)\n",
    "        label = tf.concat([z, label], axis=3)\n",
    "        return label\n",
    "    \n",
    "    input_ = _attribute_concat(label, input_)\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "                \n",
    "        for ind in list(reversed(range(len(inputs)))):\n",
    "            outout_shape = inputs[ind-1].get_shape().as_list()\n",
    "            \n",
    "            if ind==1:\n",
    "                deconv = deconv2d(input_, outout_shape, kernel_size, kernel_size, strides, strides, name = \"deconv_{}\".format(ind))\n",
    "                return tf.nn.tanh(deconv)\n",
    "            \n",
    "            deconv = deconv2d(input_, outout_shape, kernel_size, kernel_size, strides, strides, name = str(ind-1))\n",
    "            concatenated = tf.concat([deconv, inputs[ind-1]], axis=3)\n",
    "\n",
    "            batch_norm = tf.contrib.layers.batch_norm(concatenated)\n",
    "            \n",
    "            input_ = leaky_relu = tf.nn.leaky_relu(batch_norm, alpha = leakyrelu_alpha, name = \"ReLU_{}\".format(ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pBY__WKA4K9"
   },
   "source": [
    "# data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJkmV7_OA4K-"
   },
   "outputs": [],
   "source": [
    "#TO DO; self._is_eager = tf.executing_eagerly()\n",
    "#TO DO: allow growth on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "l2u4WCGnA4LC",
    "outputId": "b6211cc4-af2a-417b-c332-7bc96eb20e34"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4a3a12c69e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#tf.enable_eager_execution()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBarkley_Deep_Drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../resources/train.tfrecords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBarkley_Deep_Drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../resources/test.tfrecords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "training_data = Barkley_Deep_Drive('../resources/train.tfrecords')\n",
    "validation_data = Barkley_Deep_Drive('../resources/test.tfrecords')\n",
    "\n",
    "image_iterator, label_iterator = training_data.get_batch(BATCH_SIZE, shuffle = False)\n",
    "image_batch, label_batch = sess.run([image_iterator, label_iterator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4f7ce9b25365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_iterator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = sess.run([image_iterator, label_iterator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wlOo-tytA4LJ",
    "outputId": "ca0d7170-c334-4e63-90a1-d27c5bf25b70"
   },
   "outputs": [],
   "source": [
    "#number_of_images = np.int(np.floor(np.sqrt(BATCH_SIZE)))\n",
    "number_of_images = 3\n",
    "\n",
    "plt.figure(1, figsize=(number_of_images*2,number_of_images*2))\n",
    "for ind, i in enumerate(range(number_of_images**2)):\n",
    "    plt.subplot(number_of_images,number_of_images,i+1)\n",
    "    plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "    plt.imshow(image_batch[i])\n",
    "    plt.title(label_batch[i].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfYy8E68A4LP"
   },
   "source": [
    "# Loss definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXWMGqTsA4LQ"
   },
   "source": [
    "<img src=\"../docs/architecture.png\" style=\"height:500px\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "foGvld25A4LR",
    "outputId": "06fd7d82-666e-45a3-8d4d-ca762eafba6c"
   },
   "outputs": [],
   "source": [
    "xa = tf.placeholder(tf.float32,shape=[BATCH_SIZE,IMG_WIDTH,IMG_HEIGHT,3],name=\"xa\") #orignal image\n",
    "z = encoder(xa, reuse=tf.AUTO_REUSE) #encoder output\n",
    "\n",
    "a = tf.placeholder(tf.float32, shape=[BATCH_SIZE, NUM_CLASSES],name=\"a\") #original attributes\n",
    "b = tf.placeholder(tf.float32, shape=[BATCH_SIZE, NUM_CLASSES],name=\"b\") #desired attributes\n",
    "\n",
    "xb_hat = decoder(z, b, reuse=tf.AUTO_REUSE) #decoder output\n",
    "with tf.control_dependencies([xb_hat]):\n",
    "    xa_hat = decoder(z, a, reuse=tf.AUTO_REUSE) #decoder output\n",
    "\n",
    "xa_logit_D = discriminator(xa, reuse=tf.AUTO_REUSE) # Discriminator output -- gan\n",
    "xa_logit_C = classifier(xa, reuse=tf.AUTO_REUSE) # Classifier output -- attribute\n",
    "\n",
    "xb_logit_D = discriminator(xb_hat, reuse=tf.AUTO_REUSE) # Discriminator output -- gan\n",
    "xb_logit_C = classifier(xb_hat, reuse=tf.AUTO_REUSE) # Classifier output -- attribute basically b_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PqUnAqQA4LU"
   },
   "outputs": [],
   "source": [
    "def gradient_penalty(func, a, b):\n",
    "    \"\"\"\n",
    "    :param: discriminator (function)\n",
    "    :a: xa (tensor)\n",
    "    :b: xb_hat (tensor)\n",
    "    :return gp: gradient penalty\n",
    "    \"\"\"\n",
    "    with tf.name_scope('interpolate'):\n",
    "        alpha = tf.random_uniform(shape=a.shape, minval=0., maxval=1.)\n",
    "        inter = a + alpha * (b - a)\n",
    "    with tf.name_scope('gradient_penalty'):\n",
    "        pred = func(inter, reuse=tf.AUTO_REUSE)\n",
    "        grad = tf.gradients(pred, inter)[0]\n",
    "        norm = tf.norm(tf.contrib.slim.flatten(grad), axis=1)\n",
    "        gp = tf.reduce_mean((norm - 1.)**2)\n",
    "        return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkPnWBXQA4LY"
   },
   "source": [
    "# discriminator losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCTxp4T_A4LZ"
   },
   "outputs": [],
   "source": [
    "lambda_ = {\"3\" : 1, \"2\" : 10, \"1\" : 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrd1Sg9yA4Lb"
   },
   "outputs": [],
   "source": [
    "#losses\n",
    "loss_adv_D =  - ( tf.reduce_mean(xa_logit_D) - tf.reduce_mean(xb_logit_D) )\n",
    "\n",
    "gp = gradient_penalty(discriminator, xa, xb_hat)\n",
    "\n",
    "loss_cls_C = tf.losses.sigmoid_cross_entropy(tf.reduce_mean(a), tf.reduce_mean(xa_logit_C)) #LOSS ATTRIBUTE\n",
    " \n",
    "D_loss = gp * lambda_['3'] * loss_cls_C + loss_adv_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D83qb6NgA4Le"
   },
   "source": [
    "# generator lossses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2I_BINi_A4Ls"
   },
   "outputs": [],
   "source": [
    "loss_adv_G = -tf.reduce_mean(xb_logit_D)\n",
    "\n",
    "loss_cls_G = tf.losses.sigmoid_cross_entropy(tf.reduce_mean(b), tf.reduce_mean(xb_logit_C))\n",
    "\n",
    "loss_rec = tf.losses.absolute_difference(xa, xa_hat)\n",
    "\n",
    "G_loss =  lambda_['1'] * loss_rec + lambda_['2'] * loss_cls_G + loss_adv_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7N3bNz-OA4Lv"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0002\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Am5r3G26A4Ly"
   },
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'C_' in var.name or 'D_' in var.name]\n",
    "g_vars = [var for var in t_vars if 'G_' in var.name]\n",
    "assert(len(t_vars) == len(d_vars ) + len(g_vars )), \"mismatch in variable names\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8l4QpkRA4L4"
   },
   "outputs": [],
   "source": [
    "d_optim = tf.train.AdamOptimizer(learning_rate = BETA_1,\n",
    "                                 beta1 = BETA_1,\n",
    "                                 beta2 = BETA_2).minimize(D_loss, var_list=d_vars)\n",
    "\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate = BETA_1,\n",
    "                                 beta1 = BETA_1,\n",
    "                                 beta2 = BETA_2).minimize(G_loss, var_list=g_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFgE3duzsBch"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_eAcg8EA4L7"
   },
   "outputs": [],
   "source": [
    "def summary(tensor_collection,\n",
    "            summary_type=['mean', 'stddev', 'max', 'min', 'sparsity', 'histogram'],\n",
    "            scope=None):\n",
    "    \"\"\"Summary.\n",
    "\n",
    "    usage:\n",
    "        1. summary(tensor)\n",
    "        2. summary([tensor_a, tensor_b])\n",
    "        3. summary({tensor_a: 'a', tensor_b: 'b})\n",
    "    \"\"\"\n",
    "    def _summary(tensor, name, summary_type):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "        if name is None:\n",
    "            # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "            # session. This helps the clarity of presentation on tensorboard.\n",
    "            name = re.sub('%s_[0-9]*/' % 'tower', '', tensor.name)\n",
    "            name = re.sub(':', '-', name)\n",
    "\n",
    "        summaries = []\n",
    "        if len(tensor.shape) == 0:\n",
    "            summaries.append(tf.summary.scalar(name, tensor))\n",
    "        else:\n",
    "            if 'mean' in summary_type:\n",
    "                mean = tf.reduce_mean(tensor)\n",
    "                summaries.append(tf.summary.scalar(name + '/mean', mean))\n",
    "            if 'stddev' in summary_type:\n",
    "                mean = tf.reduce_mean(tensor)\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(tensor - mean)))\n",
    "                summaries.append(tf.summary.scalar(name + '/stddev', stddev))\n",
    "            if 'max' in summary_type:\n",
    "                summaries.append(tf.summary.scalar(name + '/max', tf.reduce_max(tensor)))\n",
    "            if 'min' in summary_type:\n",
    "                summaries.append(tf.summary.scalar(name + '/min', tf.reduce_min(tensor)))\n",
    "            if 'sparsity' in summary_type:\n",
    "                summaries.append(tf.summary.scalar(name + '/sparsity', tf.nn.zero_fraction(tensor)))\n",
    "            if 'histogram' in summary_type:\n",
    "                summaries.append(tf.summary.histogram(name, tensor))\n",
    "        return tf.summary.merge(summaries)\n",
    "\n",
    "    if not isinstance(tensor_collection, (list, tuple, dict)):\n",
    "        tensor_collection = [tensor_collection]\n",
    "\n",
    "    with tf.name_scope(scope, 'summary'):\n",
    "        summaries = []\n",
    "        if isinstance(tensor_collection, (list, tuple)):\n",
    "            for tensor in tensor_collection:\n",
    "                summaries.append(_summary(tensor, None, summary_type))\n",
    "        else:\n",
    "            for tensor, name in tensor_collection.items():\n",
    "                summaries.append(_summary(tensor, name, summary_type))\n",
    "        return tf.summary.merge(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMMbr1tjA4MK"
   },
   "outputs": [],
   "source": [
    "d_summary = summary({\n",
    "    loss_adv_D: 'loss_adv_D',\n",
    "    gp: 'gp',\n",
    "    loss_cls_C: 'loss_cls_C',\n",
    "}, scope='D_')\n",
    "\n",
    "g_summary = summary({\n",
    "    loss_adv_G: 'loss_adv_G',\n",
    "    loss_cls_G: 'loss_cls_G',\n",
    "    loss_rec: 'loss_rec',\n",
    "}, scope='G_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7E1N8bMMA4Mr"
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "summary_writer = tf.summary.FileWriter('./graphs', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3u_buBzgA4Mu"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "GENERATION_RATE = 500\n",
    "label_mapping = {'daytime': [1, 0], 'night': [0, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcEjc1-2PLKq"
   },
   "outputs": [],
   "source": [
    "for epoch_no in tqdm.tqdm(range(EPOCHS), total=EPOCHS):\n",
    "    try:\n",
    "        while True:\n",
    "            # Sample batch from dataset\n",
    "            image_batch, label_batch = sess.run([image_iterator, label_iterator])\n",
    "\n",
    "            # Transform label batch in our simple one hot encoded version\n",
    "            a_label_batch = np.asarray([label_mapping[label.decode(\"utf-8\")] for label in label_batch], dtype=np.float32)\n",
    "            b_label_batch = np.copy(a_label_batch) # Safe copy\n",
    "            b_label_batch[:,[0, 1]] = b_label_batch[:,[1, 0]] # Swapping the columns means taking the opposite!\n",
    "\n",
    "            # Execute steps\n",
    "            step_z = sess.run(z, feed_dict={xa:image_batch})\n",
    "\n",
    "            step_xa_hat = sess.run(xa_hat, feed_dict={a:a_label_batch, b:b_label_batch, xa:image_batch})\n",
    "            step_xb_hat = sess.run(xb_hat, feed_dict={a:a_label_batch, b:b_label_batch, xa:image_batch})\n",
    "\n",
    "            step_xa_logit_D = sess.run(xa_logit_D, feed_dict={xa:image_batch})\n",
    "            step_xa_logit_C = sess.run(xa_logit_C, feed_dict={xa:image_batch})\n",
    "\n",
    "            step_xb_logit_D = sess.run(xb_logit_D, feed_dict={xb_hat:step_xb_hat})\n",
    "            step_xb_logit_C = sess.run(xb_logit_C, feed_dict={xb_hat:step_xb_hat})\n",
    "            \n",
    "            # Optimize\n",
    "            d_summary_opt, _ = sess.run([d_summary, d_optim], feed_dict={xa:image_batch, xb_hat:step_xb_hat,\n",
    "                                                                        a: a_label_batch, xa_logit_C:step_xa_logit_C,\n",
    "                                                                        xa_logit_D:step_xa_logit_D, xb_logit_D:step_xb_logit_D})\n",
    "            \n",
    "            g_summary_opt, _ = sess.run([g_summary, g_optim], feed_dict={xa:image_batch, xa_hat:step_xa_hat,\n",
    "                                                                        a: a_label_batch, b: b_label_batch,\n",
    "                                                                        xb_logit_C:step_xb_logit_C,xb_logit_D:step_xb_logit_D})\n",
    "\n",
    "            # Write in the summary \n",
    "            summary_writer.add_summary(d_summary_opt, epoch_no)\n",
    "            summary_writer.add_summary(g_summary_opt, epoch_no)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "code_network_ATTNGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
