{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "id": "2mclR5a8b5Yy",
    "outputId": "dcceb331-344c-44e8-9928-1368d5017841"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "from model import *\n",
    "import argparse\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 1\n",
    "EPOCHS = 10\n",
    "GENERATION_RATE = 2\n",
    "LEARNING_RATE = 0.0002\n",
    "truncated_uniform_scale_flag = True\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 128, 128\n",
    "#BETA_1, BETA_2 = 0.5, 0.999\n",
    "BETA_1, BETA_2 = 0.5, 0.999\n",
    "thres_int = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created model\n"
     ]
    }
   ],
   "source": [
    "\"\"\" saving paths \"\"\"\n",
    "\n",
    "if model_name is None:\n",
    "    model_name = time.strftime('%Y-%m-%d_%H:%M:%S_%z') + \"_\" + str(BATCH_SIZE)\n",
    "    output_dir = \"output\"\n",
    "    model_dir = '{}/{}'.format(output_dir, model_name)\n",
    "    image_dir = '{}/images'.format(model_dir)\n",
    "    checkpoints_dir = '{}/checkpoints'.format(model_dir)\n",
    "    for path in [output_dir, model_dir, image_dir, checkpoints_dir]:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "    print(\"created model\")\n",
    "else:\n",
    "    model_name = model_name\n",
    "    print(\"proceeding to load model: {}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" tf session definitions \"\"\"\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" load TFRecordDataset \"\"\"\n",
    "training_data = Barkley_Deep_Drive('../../self-driving-AttGAN/resources/train.tfrecords')\n",
    "validation_data = Barkley_Deep_Drive('../../self-driving-AttGAN/resources/test.tfrecords')\n",
    "\n",
    "train_iterator = training_data.get_batch(EPOCHS, BATCH_SIZE, shuffle = False)\n",
    "val_iterator = validation_data.get_batch(EPOCHS, BATCH_SIZE, shuffle = False)\n",
    "\n",
    "train_image_iterator, train_label_iterator = train_iterator.get_next()\n",
    "val_image_iterator, val_label_iterator = val_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Placeholders \"\"\"\n",
    "xa = tf.placeholder(tf.float32,shape=[BATCH_SIZE,IMG_WIDTH,IMG_HEIGHT,3],name=\"xa\") #orignal image\n",
    "z = encoder(xa, reuse=tf.compat.v1.AUTO_REUSE ) #encoder output\n",
    "\n",
    "a = tf.placeholder(tf.float32, shape=[BATCH_SIZE, NUM_CLASSES],name=\"a\") #original attributes\n",
    "b = tf.placeholder(tf.float32, shape=[BATCH_SIZE, NUM_CLASSES],name=\"b\") #desired attributes\n",
    "\n",
    "xb_hat = decoder(z, b, reuse=tf.compat.v1.AUTO_REUSE ) #decoder output\n",
    "with tf.control_dependencies([xb_hat]):\n",
    "    xa_hat = decoder(z, a, reuse=tf.compat.v1.AUTO_REUSE ) #decoder output\n",
    "\n",
    "xa_logit_D, xa_logit_C = classifier_and_discriminator(xa, reuse=tf.compat.v1.AUTO_REUSE,  NUM_CLASSES=1)\n",
    "xb_logit_D, xb_logit_C = classifier_and_discriminator(xb_hat, reuse=tf.compat.v1.AUTO_REUSE,  NUM_CLASSES=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" penalty \"\"\"\n",
    "lambda_ = {\"3\" : 1, \"2\" : 10, \"1\" : 100}\n",
    "\n",
    "\"\"\" interpolated image noise\"\"\"\n",
    "#epsilon = tf.random_uniform(shape=[BATCH_SIZE, 1, 1, 1], minval=0., maxval=1.)\n",
    "#interpolated_image = xa + epsilon * (xb_hat - xa)\n",
    "#c_interpolated, d_interpolated = classifier_and_discriminator(interpolated_image, reuse=tf.compat.v1.AUTO_REUSE, NUM_CLASSES=1)\n",
    "\n",
    "# Gradient penalty\n",
    "alpha = tf.random_uniform(\n",
    "    shape=[IMG_WIDTH,1], \n",
    "    minval=0.,\n",
    "    maxval=1.\n",
    ")\n",
    "differences =  xb_hat - xa\n",
    "interpolates = xa + (alpha*differences)\n",
    "gradients = tf.gradients(classifier_and_discriminator(interpolates, reuse=tf.AUTO_REUSE), [interpolates])[0]\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "gp = tf.reduce_mean((slopes-1.)**2)\n",
    "\n",
    "\"\"\" D loss \"\"\"\n",
    "loss_adv_D =  - ( tf.reduce_mean(xa_logit_D) - tf.reduce_mean(xb_logit_D) )\n",
    "#grad_d_interpolated = tf.gradients(d_interpolated, [interpolated_image])[0]\n",
    "#slopes = tf.sqrt(1e-10 + tf.reduce_sum(tf.square(grad_d_interpolated), axis=[1, 2, 3]))\n",
    "#gp = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "\n",
    "loss_cls_C = tf.losses.sigmoid_cross_entropy(a, xa_logit_C)\n",
    "\n",
    "D_loss = loss_adv_D + gp * lambda_['2'] + loss_cls_C\n",
    "\n",
    "\"\"\" G loss \"\"\"\n",
    "loss_adv_G = -tf.reduce_mean(xb_logit_D)\n",
    "loss_cls_G = tf.losses.sigmoid_cross_entropy(b, xb_logit_C)\n",
    "loss_rec = tf.losses.absolute_difference(xa, xa_hat)\n",
    "\n",
    "G_loss =  loss_adv_G + lambda_['2'] * loss_cls_G + lambda_['1'] * loss_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not load checkpoints\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# divide trainable variables into a group for D and a group for G\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'C_D' in var.name ]\n",
    "g_vars = [var for var in t_vars if 'G_' in var.name]\n",
    "assert(len(t_vars) == len(d_vars ) + len(g_vars )), \"mismatch in variable names\"\n",
    "\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE,\n",
    "                                 beta1 = BETA_1,\n",
    "                                 beta2 = BETA_2).minimize(D_loss, var_list=d_vars)\n",
    "\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE,\n",
    "                                 beta1 = BETA_1,\n",
    "                                 beta2 = BETA_2).minimize(G_loss, var_list=g_vars)\n",
    "\n",
    "\"\"\" Summary \"\"\"\n",
    "d_summary = summary({\n",
    "    loss_adv_D: 'loss_adv_D',\n",
    "    gp: 'gp',\n",
    "    loss_cls_C: 'loss_cls_C',\n",
    "}, scope='D_')\n",
    "\n",
    "g_summary = summary({\n",
    "    loss_adv_G: 'loss_adv_G',\n",
    "    loss_cls_G: 'loss_cls_G',\n",
    "    loss_rec: 'loss_rec',\n",
    "}, scope='G_')\n",
    "\n",
    "\"\"\" init \"\"\"\n",
    "summary_writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "S_ = tf.Summary()\n",
    "\n",
    "\"\"\" checkpoints load \"\"\"\n",
    "try:\n",
    "    load_checkpoint(checkpoints_dir, sess, t_vars)\n",
    "except:\n",
    "    print(\"did not load checkpoints\")\n",
    "    sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" mapping defintions \"\"\"\n",
    "d_loss_epoch, g_loss_epoch = [], []\n",
    "if NUM_CLASSES == 2:\n",
    "    label_mapping = {'daytime': [1, 0], 'night': [0, 1]}\n",
    "else:\n",
    "    label_mapping = {'daytime': [1], 'night': [0]}\n",
    "    flip = {'1':[0], '0': [1] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 we have\n",
      "Gen loss:  51.41764  and Desc loss: 7.2865767 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -0.01121532917022705\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.6553004384040833\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.7447875142097473\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 6.498315811157227\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.7086101174354553\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.378332257270813\n",
      "}\n",
      "\n",
      "At step  400 we have\n",
      "Gen loss:  27.456  and Desc loss: 6.060248 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: 0.486125111579895\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.9373887181282043\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.8757617473602295\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 0.8416420221328735\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.7507617473602295\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.07500743865966797\n",
      "}\n",
      "\n",
      "At step  800 we have\n",
      "Gen loss:  24.174932  and Desc loss: 2.4679976 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -63.654930114746094\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.2566511332988739\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 32.45002746582031\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.10455843806266785\n",
      "}\n",
      "\n",
      "At step  1200 we have\n",
      "Gen loss:  30.653801  and Desc loss: -2.7670403 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: 1.4267215728759766\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.7783460021018982\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 12.458980560302734\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.0697367712855339\n",
      "}\n",
      "\n",
      "At step  1600 we have\n",
      "Gen loss:  25.245462  and Desc loss: -2.095611 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -76.67338562011719\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.30836251378059387\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 50.16497039794922\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.12813052535057068\n",
      "}\n",
      "\n",
      "At step  2000 we have\n",
      "Gen loss:  30.394747  and Desc loss: -16.21932 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -185.90682983398438\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.9150900840759277\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 95.68842315673828\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.10247892141342163\n",
      "}\n",
      "\n",
      "At step  2400 we have\n",
      "Gen loss:  47.022156  and Desc loss: -47.630104 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -272.7110595703125\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 7.5917510986328125\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 148.39971923828125\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.1301286518573761\n",
      "}\n",
      "\n",
      "At step  2800 we have\n",
      "Gen loss:  49.758686  and Desc loss: -54.59476 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -354.9295654296875\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 17.949804306030273\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 152.42056274414062\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.12054547667503357\n",
      "}\n",
      "\n",
      "Model is saved at output/2019-12-08_19:33:36_+0000_32/checkpoints/Epoch_0_2870.ckpt!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [05:56<53:27, 356.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH\n",
      "Epoch duration: 0:05:56.059150\n",
      "Discriminator loss:  -56.918068\n",
      "Generator loss:  53.121864\n",
      "------------------------------\n",
      "At step  0 we have\n",
      "Gen loss:  176.04536  and Desc loss: -101.17202 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -109.17130279541016\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.7306140065193176\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 153.3275146484375\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.1578637808561325\n",
      "}\n",
      "\n",
      "At step  400 we have\n",
      "Gen loss:  -82.000854  and Desc loss: -10.937937 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: 3.6623382568359375\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.8960720896720886\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.648134708404541\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: -124.4700927734375\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.7900403738021851\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.08009111136198044\n",
      "}\n",
      "\n",
      "At step  800 we have\n",
      "Gen loss:  -92.77544  and Desc loss: -0.7966522 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -3.434234619140625\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.7168576121330261\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: -98.53077697753906\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.0794287919998169\n",
      "}\n",
      "\n",
      "At step  1200 we have\n",
      "Gen loss:  -67.21233  and Desc loss: -2.4621336 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -1.2826919555664062\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.48823079466819763\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: -91.5846939086914\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.08355385065078735\n",
      "}\n",
      "\n",
      "At step  1600 we have\n",
      "Gen loss:  -23.676243  and Desc loss: -20.402456 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -17.2584228515625\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.5151737332344055\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: -74.4088363647461\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.10575178265571594\n",
      "}\n",
      "\n",
      "At step  2000 we have\n",
      "Gen loss:  3.3782248  and Desc loss: -39.945786 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -489.44683837890625\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.33382511138916016\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 292.6327819824219\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.19190621376037598\n",
      "}\n",
      "\n",
      "At step  2400 we have\n",
      "Gen loss:  35.36285  and Desc loss: -67.908775 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -28.738388061523438\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.3455939292907715\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 274.49798583984375\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.10853945463895798\n",
      "}\n",
      "\n",
      "At step  2800 we have\n",
      "Gen loss:  48.5357  and Desc loss: -70.607315 \n",
      " \n",
      "value {\n",
      "  tag: \"D_/loss_adv_D\"\n",
      "  simple_value: -50.30763244628906\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/gp\"\n",
      "  simple_value: 0.823976993560791\n",
      "}\n",
      "value {\n",
      "  tag: \"D_/loss_cls_C\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "\n",
      "value {\n",
      "  tag: \"G_/loss_adv_G\"\n",
      "  simple_value: 222.2947540283203\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_cls_G\"\n",
      "  simple_value: 0.6931471824645996\n",
      "}\n",
      "value {\n",
      "  tag: \"G_/loss_rec\"\n",
      "  simple_value: 0.09362813085317612\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" main training loop \"\"\"\n",
    "\n",
    "for epoch_no in tqdm.tqdm(range(EPOCHS), total=EPOCHS):\n",
    "\n",
    "    sess.run(val_iterator.initializer)\n",
    "    sess.run(train_iterator.initializer)\n",
    "\n",
    "    try:\n",
    "        step = 0\n",
    "        start_time = time.monotonic()\n",
    "        d_loss_per_batch = []\n",
    "        g_loss_per_batch = []\n",
    "        while True:\n",
    "            # Sample batch from dataset\n",
    "            image_batch, label_batch = sess.run([val_image_iterator, val_label_iterator])\n",
    "            #image_batch, label_batch = sess.run([train_image_iterator, train_label_iterator])\n",
    "\n",
    "            # Transform label batch in our simple one hot encoded version\n",
    "            a_label_batch = np.array([label_mapping[label.decode(\"utf-8\")] for label in label_batch], dtype=np.float32)\n",
    "            if truncated_uniform_scale_flag:\n",
    "                b_label_batch = a_label_batch.copy().astype(np.float32)\n",
    "                np.random.shuffle(b_label_batch)\n",
    "                a_label_batch = (a_label_batch * 2 - 1) * thres_int\n",
    "                b_label_batch = (b_label_batch * 2 - 1) * (np.random.uniform(size=b_label_batch.shape) + 2) / 4.0 * (2 * thres_int)\n",
    "            else:\n",
    "                b_label_batch = [flip[str(int(label))] for label in a_label_batch]\n",
    "                a_label_batch = np.asarray(a_label_batch, dtype=np.float32)\n",
    "                b_label_batch = np.asarray(b_label_batch, dtype=np.float32)\n",
    "\n",
    "            # Optimize\n",
    "            d_summary_opt, _, D_loss_val = sess.run([d_summary, d_optim, D_loss], feed_dict={xa:image_batch,\n",
    "                                                                    a: a_label_batch, b: b_label_batch})\n",
    "            if step % 5 == 0:\n",
    "                g_summary_opt, _, G_loss_val = sess.run([g_summary, g_optim, G_loss], feed_dict={xa:image_batch,\n",
    "                                                                    a: a_label_batch, b: b_label_batch})\n",
    "\n",
    "            d_loss_per_batch.append(D_loss_val)\n",
    "            g_loss_per_batch.append(G_loss_val)\n",
    "\n",
    "            summary_writer.add_summary(d_summary_opt, epoch_no)\n",
    "            summary_writer.add_summary(g_summary_opt, epoch_no)\n",
    "\n",
    "            if step % 400 == 0:\n",
    "                print(\"At step \", step, \"we have\")\n",
    "                print(\"Gen loss: \",np.mean(g_loss_per_batch), \" and Desc loss:\", np.mean(d_loss_per_batch) , \"\\n \")\n",
    "                S_.ParseFromString(d_summary_opt)\n",
    "                print(S_)\n",
    "                S_.ParseFromString(g_summary_opt)\n",
    "                print(S_)\n",
    "\n",
    "            step += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "            checkpoint_save_path = saver.save(sess, '{}/Epoch_{}_{}.ckpt'.format(checkpoints_dir, str(epoch_no), str(step)))\n",
    "            print('Model is saved at {}!'.format(checkpoint_save_path))\n",
    "\n",
    "            # Generating reconstructed image xa_hat and flipped attribute image xb_hat\n",
    "            #image_batch, label_batch = sess.run([val_image_iterator, val_label_iterator])\n",
    "            image_batch, label_batch = sess.run([train_image_iterator, train_label_iterator])\n",
    "\n",
    "            # Transform label batch in our simple one hot encoded version\n",
    "            a_label_batch = np.array([label_mapping[label.decode(\"utf-8\")] for label in label_batch])#, dtype=np.float64)\n",
    "            if truncated_uniform_scale_flag:\n",
    "                b_label_batch = tf.random_shuffle(a_label_batch)\n",
    "                a_label_batch = (a_label_batch * 2 - 1) * thres_int\n",
    "                b_label_batch = (tf.to_float(b_label_batch) * 2 - 1) * (tf.truncated_normal(tf.shape(b_label_batch)) + 2) / 4.0 * (2 * thres_int)\n",
    "                b_label_batch = sess.run(b_label_batch)\n",
    "            else:\n",
    "                b_label_batch = [flip[str(int(label))] for label in a_label_batch]\n",
    "                a_label_batch = np.asarray(a_label_batch, dtype=np.float32)\n",
    "                b_label_batch = np.asarray(b_label_batch, dtype=np.float32)\n",
    "\n",
    "            step_xb_hat = sess.run(xb_hat, feed_dict={a:a_label_batch, b:b_label_batch, xa:image_batch})\n",
    "            step_xa_hat = sess.run(xa_hat, feed_dict={a:a_label_batch, b:b_label_batch, xa:image_batch})\n",
    "            \n",
    "            \"\"\"image saving\"\"\"\n",
    "            output_path = os.path.join(image_dir, \"epoch_no_\"+ str(epoch_no) +\"_\" +\".png\")\n",
    "            plot_block_after_epoch(output_path, label_batch, image_batch, step_xa_hat, step_xb_hat,\n",
    "                                   examples = 3, plot = True)\n",
    "            \n",
    "\n",
    "            end_time = time.monotonic()\n",
    "            print(\"END OF EPOCH\")\n",
    "            fmt = \"Epoch duration: {}\".format(timedelta(seconds=end_time - start_time))\n",
    "            print(fmt)\n",
    "            d_loss_epoch.append(np.mean(d_loss_per_batch))\n",
    "            g_loss_epoch.append(np.mean(g_loss_per_batch))\n",
    "            print(\"Discriminator loss: \", d_loss_epoch[-1])\n",
    "            print(\"Generator loss: \", g_loss_epoch[-1])\n",
    "            print(\"-\"*len(fmt))\n",
    "            pass\n",
    "\n",
    "\n",
    "checkpoint_save_path = saver.save(sess, '{}/Epoch_{}_{}.ckpt'.format(checkpoints_dir, str(epoch_no), str(step)))\n",
    "print('Finished training\\nModel has been saved at {}!'.format(checkpoint_save_path))\n",
    "sess.close()\n",
    "\n",
    "try:\n",
    "    to_json = {\"d_loss\": d_loss_epoch,\n",
    "           \"g_loss\": g_loss_epoch}\n",
    "    with open(model_name+'.json', 'w') as f:\n",
    "        json.dump(to_json, f)\n",
    "except:\n",
    "    print(d_loss_epoch)\n",
    "    print(g_loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "network-ATTNGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
